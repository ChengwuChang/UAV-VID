#ICP2D
#繪製三角形
import numpy as np
import cv2 as cv
import open3d as o3d
import math
from simpleicp import PointCloud, SimpleICP
from scipy.spatial import KDTree

def icp_2d(X, Y, max_iterations=100, tolerance=1e-5):
    """
    Perform ICP (Iterative Closest Point) algorithm in 2D.

    Parameters:
    X : numpy.ndarray
        The source point set (N x 2).
    Y : numpy.ndarray
        The target point set (N x 2).
    max_iterations : int, optional
        Maximum number of iterations. Default is 100.
    tolerance : float, optional
        Convergence criteria. Default is 1e-5.

    Returns:
    R : numpy.ndarray
        The rotation matrix (2 x 2).
    T : numpy.ndarray
        The translation vector (2,).
    """
    # Initialize transformation
    R = np.eye(2)
    T = np.zeros(2)

    for _ in range(max_iterations):
        # Find nearest neighbors
        tree = KDTree(Y)
        distances, indices = tree.query(X)

        # Compute centroid
        X_centroid = np.mean(X, axis=0)
        Y_centroid = np.mean(Y[indices], axis=0)

        # Compute covariance matrix
        H = np.dot((X - X_centroid).T, (Y[indices] - Y_centroid))

        # Singular Value Decomposition
        U, _, Vt = np.linalg.svd(H)

        # Calculate rotation matrix
        R = np.dot(U, Vt)

        # Calculate translation vector
        T = Y_centroid - np.dot(R, X_centroid)

        # Apply transformation to source points
        X_transformed = np.dot(X, R.T) + T

        # Check convergence
        if np.all(np.abs(X_transformed - Y[indices]) < tolerance):
            break

        # Update source points
        X = X_transformed

    return R, T

def save_xy_file(point_cloud, filename):
    # 提取点云数据的 x 和 y 坐标
    points_xy = np.asarray(point_cloud.points)[:, :2]

    # 将点云数据保存到 .xy 文件
    with open(filename, "w") as file:
        for point in points_xy:
            # 将每个点的 x、y 坐标写入文件，每行一个点
            file.write(f"{point[0]} {point[1]}\n")

def rotation_matrix_to_angle(R):
    angle = np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi
    return angle

def compute_rotation_angle(img1, img2):
    # 初始化 SIFT 檢測器
    sift = cv.SIFT_create()

    # 使用 SIFT 找到關鍵點和描述子
    kp1, des1 = sift.detectAndCompute(img1, None)
    kp2, des2 = sift.detectAndCompute(img2, None)

    # 定義 FLANN 參數
    FLANN_INDEX_KDTREE = 1
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    search_params = dict(checks=50)
    flann = cv.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(des1, des2, k=2)

    good = []
    for m, n in matches:
        if m.distance < 0.7 * n.distance:
            good.append(m)

    # 獲取匹配點的坐標
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

    # 計算兩組點的旋轉矩陣
    M, _ = cv.estimateAffine2D(src_pts, dst_pts)

    # 從旋轉矩陣中提取旋轉角度
    rotation_angle = np.arctan2(M[1, 0], M[0, 0]) * 180 / np.pi

    return rotation_angle

# Function to rotate a point (x, y) around another point (cx, cy) by angle theta (in degrees)
def rotate_point(x, y, cx, cy, angle):
    # Convert angle to radians
    theta = math.radians(angle)
    # Perform rotation
    x_rotated = cx + (x - cx) * math.cos(theta) - (y - cy) * math.sin(theta)
    y_rotated = cy + (x - cx) * math.sin(theta) + (y - cy) * math.cos(theta)
    return x_rotated, y_rotated


MIN_MATCH_COUNT = 10

# 讀取img3
img3 = cv.imread("C:\\Users\\small\\yolov5-master\\0313\\jpg\\14\\23\\23-1.jpg", 1) # trainImage

# 初始化 SIFT 檢測器
sift = cv.SIFT_create()

# 使用 SIFT 找到img3的關鍵點和描述子
kp3, des3 = sift.detectAndCompute(img3, None)

# 定義 FLANN 參數
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)

# 創建 FLANN 匹配器
flann = cv.FlannBasedMatcher(index_params, search_params)

# 儲存所有方框的座標
all_boxes = []

# 儲存所有中心點座標
center_points = []

#target特徵點坐標
target_points = []
#儲存特徵點坐標
matched_points = []

# 讀取10張圖片
for i in range(1, 21):
    # 讀取影像
    img = cv.imread(f'output_frames/frame_{i}.jpg', 1)  # queryImage

    # 使用 SIFT 找到關鍵點和描述子
    kp, des = sift.detectAndCompute(img, None)

    # 進行特徵匹配
    matches = flann.knnMatch(des, des3, k=2)

    # 根據 Lowe's ratio 測試存儲所有良好的匹配
    good = []
    for m, n in matches:
        if m.distance < 0.7 * n.distance:
            good.append(m)

    # 執行 Homography 變換
    if len(good) > MIN_MATCH_COUNT:
        src_pts = np.float32([kp[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp3[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

        M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)
        matchesMask = mask.ravel().tolist()

        h, w, d = img.shape
        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
        dst = cv.perspectiveTransform(pts, M)
        x1, y1 = np.int32(dst[0][0])
        x2, y2 = np.int32(dst[1][0])
        x3, y3 = np.int32(dst[2][0])
        x4, y4 = np.int32(dst[3][0])
        center_x = (x1 + x2 + x3 + x4) / 4
        center_y = (y1 + y2 + y3 + y4) / 4
        # print("中心坐標為:", center_x, center_y)
        center_points.append((center_x, center_y))
        matched_points.append(src_pts.squeeze())
        target_points.append(dst_pts.squeeze())
        all_boxes.append(np.int32(dst))
        # 將特徵點的 x 和 y 坐標添加到點雲中，並將 z 坐標設置為 0
        point_cloud = o3d.geometry.PointCloud()
        target_point_cloud = o3d.geometry.PointCloud()
        for points in matched_points:
            # 將每個特徵點轉換為 3D 點
            points_3d = np.hstack((points, np.zeros((points.shape[0], 1), dtype=np.float32)))
            # 將 3D 點添加到點雲中
            point_cloud.points.extend(o3d.utility.Vector3dVector(points_3d))
        for points in target_points:
            # 將每個特徵點轉換為 3D 點
            target_points_3d = np.hstack((points, np.zeros((points.shape[0], 1), dtype=np.float32)))
            # 將 3D 點添加到點雲中
            target_point_cloud.points.extend(o3d.utility.Vector3dVector(target_points_3d))

        save_xy_file(point_cloud, 'point_cloud.xy')
        save_xy_file(target_point_cloud, 'target_point_cloud.xy')

        X = np.loadtxt("point_cloud.xy ")
        Y = np.loadtxt("target_point_cloud.xy")
        # Run ICP
        R, T = icp_2d(X, Y)

        angle = rotation_matrix_to_angle(R)
        #angle = compute_rotation_angle(img3, img)

        # Calculate vertices of the isosceles triangle around each center point
        for center_point in center_points:
            center_x, center_y = map(int, center_point)
            base_length = 50
            half_base = base_length // 2
            height = int(0.866 * base_length)  # Height of an equilateral triangle is sqrt(3)/2 times the base length
            offset_factor = 50  # Adjust this value as needed
            # offset = offset_factor * angle / abs(angle) if angle != 0 else offset_factor

            # Calculate coordinates of base vertices
            vertex1 = (int(center_x - half_base - offset_factor), int(center_y + height))
            vertex2 = (int(center_x - offset_factor), int(center_y - height))
            vertex3 = (int(center_x + half_base - offset_factor), int(center_y + height))

            # Rotate the base vertices around the center point by a given angle (e.g., 45 degrees)
            # rotation_angle = 45  # Specify the rotation angle here
            vertex1_rotated = rotate_point(vertex1[0], vertex1[1], center_x, center_y, angle)
            vertex2_rotated = rotate_point(vertex2[0], vertex2[1], center_x, center_y, angle)
            vertex3_rotated = rotate_point(vertex3[0], vertex3[1], center_x, center_y, angle)

            # Draw the rotated triangle
            vertices_rotated = np.array([vertex1_rotated, vertex2_rotated, vertex3_rotated], dtype=np.int32)
            cv.fillPoly(img3, [vertices_rotated], color=(0, 255, 255))

        print(f"Frame {i + 1} Rotation angle (degrees):", angle)
        print("Rotation matrix:")
        print(R)
        print("Translation vector:")
        print(T)

    else:
        print(f"Not enough matches are found in frame {i} - {len(good)}/{MIN_MATCH_COUNT}")
# for point in center_points:
#     print(point)
# 將所有方框繪製到img3上
color = (0, 255, 0)  # BGR
color2 = (0, 0, 255) # BGR

# # 將點雲保存為 XYZ 檔
# o3d.io.write_point_cloud("feature_points.xyz", point_cloud)
# o3d.io.write_point_cloud("target_feature_points.xyz", target_point_cloud)
# # 讀取原始特徵點點雲&目標特徵點點雲
#
# # Read point clouds from xyz files into n-by-3 numpy arrays
# X_fix = np.genfromtxt("target_feature_points.xyz")
# X_mov = np.genfromtxt("feature_points.xyz")
#
# # Create point cloud objects
# pc_fix = PointCloud(X_fix, columns=["x", "y", "z"])
# pc_mov = PointCloud(X_mov, columns=["x", "y", "z"])
#
# # Create simpleICP object, add point clouds, and run algorithm!
# icp = SimpleICP()
# icp.add_point_clouds(pc_fix, pc_mov)
# H, X_mov_transformed, rigid_body_transformation_params, distance_residuals = icp.run(max_overlap_distance=1)

# print(matched_points)
# print(target_points)

for box in all_boxes:
    img3 = cv.polylines(img3, [box], True, color, 3, cv.LINE_AA)
# 將所有中心點標示出來
for point in center_points:
    center_x, center_y = map(int, point)
    # cv.circle(img3, (center_x, center_y), radius=5, color=(255, 255, 255), thickness=25)
# 將所有中心點連接成直線
for i in range(len(center_points) - 1):
    cv.line(img3, tuple(map(int, center_points[i])), tuple(map(int, center_points[i + 1])), color2, 10)
# 顯示結果
cv.namedWindow("All Boxes", cv.WINDOW_NORMAL)
cv.imshow("All Boxes", img3)
cv.waitKey(0)
cv.destroyAllWindows()
