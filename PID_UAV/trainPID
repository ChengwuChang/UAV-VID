from stable_baselines3 import PPO
from Pid_RL import PIDTuningEnv
import json
import numpy as np

# å»ºç«‹ç’°å¢ƒ
env = PIDTuningEnv(gui=False)

# å»ºç«‹ PPO æ¨¡å‹
model = PPO("MlpPolicy", env, verbose=0)

# åˆ†æ®µè¨“ç·´ä¸¦é¡¯ç¤ºåƒæ•¸
TOTAL_SEGMENTS = 10
STEPS_PER_SEGMENT = 5

for i in range(TOTAL_SEGMENTS):
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´å€æ®µ {i+1}/{TOTAL_SEGMENTS}ï¼ˆå…± {STEPS_PER_SEGMENT} æ­¥ï¼‰")
    model.learn(total_timesteps=STEPS_PER_SEGMENT, reset_num_timesteps=False)
    obs = env.reset()
    action, _ = model.predict(obs)
    kp, ki, kd, lookahead = action
    print(f"ğŸ¯ æ¨æ¸¬åƒæ•¸ï¼šKP={kp:.3f}, KI={ki:.3f}, KD={kd:.3f}, Lookahead={int(round(lookahead))}")

# æœ€çµ‚è¼¸å‡ºæœ€ä½³åƒæ•¸
obs = env.reset()
action, _ = model.predict(obs)
kp, ki, kd, lookahead = action
lookahead = int(round(lookahead))

print("\nâœ… å¼·åŒ–å­¸ç¿’è¨“ç·´å®Œæˆ")
print(f"KP = {kp:.4f}, KI = {ki:.4f}, KD = {kd:.4f}, Lookahead = {lookahead}")

# å„²å­˜åƒæ•¸
best_params = {
    "KP_XY": float(kp),
    "KI_XY": float(ki),
    "KD_XY": float(kd),
    "Lookahead": lookahead
}

with open("best_pid_params.json", "w") as f:
    json.dump(best_params, f, indent=2)

print("\nğŸ“ æœ€ä½³åƒæ•¸å·²å„²å­˜è‡³ best_pid_params.json")
